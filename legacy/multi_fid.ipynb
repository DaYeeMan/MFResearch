{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b52b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def normalize_data(data, qh_min, qh_max):\n",
    "  \"\"\"\n",
    "  Normalizes data based on the high-fidelity data range (min and max).\n",
    "\n",
    "  Args:\n",
    "    data (np.ndarray): The data to normalize (can be scalar or array).\n",
    "    qh_min (float): The minimum value of the high-fidelity training data (Q_H).\n",
    "    qh_max (float): The maximum value of the high-fidelity training data (Q_H).\n",
    "\n",
    "  Returns:\n",
    "    np.ndarray: The normalized data, scaled to the range  relative to\n",
    "                the high-fidelity range. Returns NaN if qh_max equals qh_min.\n",
    "  \"\"\"\n",
    "  if qh_max == qh_min:\n",
    "      # Handle case of zero range to avoid division by zero\n",
    "      # Option 1: Return 0.5 for all values (midpoint)\n",
    "      # Option 2: Return 0 for all values\n",
    "      # Option 3: Raise an error\n",
    "      # Current implementation: Return NaN or handle as appropriate for the application\n",
    "      return np.full_like(data, np.nan, dtype=np.float64)\n",
    "  return (data - qh_min) / (qh_max - qh_min)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc33d270",
   "metadata": {},
   "outputs": [],
   "source": [
    "def denormalize_data(normalized_data, qh_min, qh_max):\n",
    "  \"\"\"\n",
    "  Denormalizes data that was previously normalized using the high-fidelity range.\n",
    "\n",
    "  Args:\n",
    "    normalized_data (np.ndarray): The normalized data (typically model output).\n",
    "    qh_min (float): The minimum value used for normalization.\n",
    "    qh_max (float): The maximum value used for normalization.\n",
    "\n",
    "  Returns:\n",
    "    np.ndarray: The data denormalized back to the original scale.\n",
    "  \"\"\"\n",
    "  # If data was normalized when qh_min == qh_max (resulting in NaN or fixed value),\n",
    "  # denormalization might return qh_min or handle based on the normalization choice.\n",
    "  if qh_max == qh_min:\n",
    "      # If normalization returned NaN, denormalization might also return NaN\n",
    "      # or return the constant value qh_min.\n",
    "      # Let's assume returning qh_min is desired if the range was zero.\n",
    "      # Check if normalized_data is NaN if that was the normalization output\n",
    "      if np.any(np.isnan(normalized_data)):\n",
    "           return np.full_like(normalized_data, np.nan, dtype=np.float64)\n",
    "      # Otherwise, assume normalization produced a fixed value (e.g., 0 or 0.5)\n",
    "      # and denormalizing should yield the single HF value.\n",
    "      return np.full_like(normalized_data, qh_min, dtype=np.float64)\n",
    "\n",
    "  return normalized_data * (qh_max - qh_min) + qh_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972d1265",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_input_table(hf_y, hf_q_l, lf_y, lf_q):\n",
    "    \"\"\"\n",
    "    Compiles the structured input table for a single high-fidelity point.\n",
    "\n",
    "    Args:\n",
    "        hf_y (float or np.ndarray): Input parameter(s) for the high-fidelity point.\n",
    "        hf_q_l (float): The *normalized* low-fidelity prediction at hf_y.\n",
    "        lf_y (np.ndarray): Low-fidelity input parameters. Shape (N_L,) or (N_L, D_y).\n",
    "        lf_q (np.ndarray): Low-fidelity predictions. Shape (N_L,).\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: The compiled input table, shape (N_L, C).\n",
    "    \"\"\"\n",
    "    # Debug: Print input shapes\n",
    "    print(f\"hf_y: {hf_y}, hf_y shape: {np.shape(hf_y)}\")\n",
    "    print(f\"lf_y shape: {lf_y.shape}, lf_q shape: {lf_q.shape}\")\n",
    "\n",
    "    n_l = lf_y.shape[0]  # Number of low-fidelity samples\n",
    "\n",
    "    # Ensure hf_y is treated as an array\n",
    "    hf_y_arr = np.atleast_1d(hf_y)\n",
    "    d_y = hf_y_arr.shape[0]  # Dimensionality of hf_y\n",
    "\n",
    "    # Ensure lf_y is 2D if D_y > 1, or reshape 1D lf_y to (N_L, 1)\n",
    "    if d_y > 1:\n",
    "        if lf_y.ndim == 1:\n",
    "            lf_y_arr = lf_y.reshape(-1, 1)\n",
    "        elif lf_y.ndim == 2 and lf_y.shape[1] == d_y:\n",
    "            lf_y_arr = lf_y\n",
    "        else:\n",
    "            raise ValueError(f\"lf_y shape {lf_y.shape} incompatible with hf_y dimension {d_y}\")\n",
    "    else:\n",
    "        lf_y_arr = lf_y.reshape(-1, 1)\n",
    "\n",
    "    # Ensure lf_q is reshaped to (N_L, 1)\n",
    "    lf_q_arr = lf_q.reshape(-1, 1)\n",
    "\n",
    "    # Debug: Print reshaped arrays\n",
    "    print(f\"lf_y_arr shape: {lf_y_arr.shape}, lf_q_arr shape: {lf_q_arr.shape}\")\n",
    "\n",
    "    # Repeat hf_y and hf_q_l for all N_L rows\n",
    "    hf_y_repeated = np.tile(hf_y_arr, (n_l, 1))\n",
    "    hf_q_l_repeated = np.full((n_l, 1), hf_q_l)\n",
    "\n",
    "    # Debug: Print repeated arrays\n",
    "    print(f\"hf_y_repeated shape: {hf_y_repeated.shape}, hf_q_l_repeated shape: {hf_q_l_repeated.shape}\")\n",
    "\n",
    "    # Assemble the table\n",
    "    input_table = np.hstack((lf_y_arr, lf_q_arr, hf_y_repeated, hf_q_l_repeated))\n",
    "    #input_table = np.hstack((lf_y_arr, lf_q_arr, hf_y_repeated))\n",
    "\n",
    "    # Debug: Print final table shape\n",
    "    print(f\"input_table shape: {input_table.shape}\")\n",
    "\n",
    "    # Check for standard 1D case\n",
    "    if np.isscalar(hf_y) and input_table.shape[1] != 4:\n",
    "        print(f\"Warning: Expected shape (N_L, 4) for scalar hf_y, but got {input_table.shape}\")\n",
    "\n",
    "    return input_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "994685d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "def build_mda_cnn(input_table_shape, num_filters=64, kernel_size=30, dnn_hidden_units=10):\n",
    "  \"\"\"\n",
    "  Builds and compiles the MDA-CNN model.\n",
    "\n",
    "  Args:\n",
    "    input_table_shape (tuple): Shape of a single input table (N_L, C),\n",
    "                               e.g., (100, 4) for N_L=100, C=4 channels.\n",
    "    num_filters (int): Number of filters for the Conv1D layer.\n",
    "    kernel_size (int): Kernel size for the Conv1D layer.\n",
    "    dnn_hidden_units (int): Number of units in each hidden Dense layer.\n",
    "\n",
    "  Returns:\n",
    "    keras.Model: The compiled Keras model.\n",
    "  \"\"\"\n",
    "  # Input Layer\n",
    "  input_tensor = keras.Input(shape=input_table_shape, name='Input_Table')\n",
    "\n",
    "  # Convolutional Layer\n",
    "  # Using 'same' padding ensures the output length is N_L.\n",
    "  # 'relu' activation is a common choice for Conv layers.\n",
    "  conv_layer0 = layers.Conv1D(filters=num_filters,\n",
    "                             kernel_size=kernel_size,\n",
    "                             strides=1,\n",
    "                             padding='same', # Ensures output sequence length = N_L\n",
    "                             activation='tanh', # 'relu' is common, but 'tanh' can also be used\n",
    "                             name='Conv1D_Layer0')(input_tensor)\n",
    "  # Second Convolutional Layer\n",
    "  # This layer can be used to further extract features from the output of the first Conv1D layer.\n",
    "  # Change padding to 'causal' for \"half\" padding effect (left-padding).\n",
    "  # Note: Keras does not have a 'half' padding mode, but 'causal' is the closest for 1D convs.\n",
    "  # If you want true \"half\" padding (as in some frameworks), you may need to implement a custom layer.\n",
    "  conv_layer1 = layers.Conv1D(filters=num_filters,\n",
    "                             kernel_size=kernel_size,\n",
    "                             strides=2,  # Step size for moving the filter along the input\n",
    "                             padding='same', # 'causal' gives left-side padding (like \"half\" in some contexts)\n",
    "                             activation='relu', # 'relu' is common, but 'tanh' can also be used\n",
    "                             name='Conv1D_Layer1')(conv_layer0)\n",
    "  # conv_layer2 = layers.Conv1D(filters=num_filters,\n",
    "  #                            kernel_size=kernel_size,\n",
    "  #                            strides=1,  # Step size for moving the filter along the input\n",
    "  #                            padding='same', # 'causal' gives left-side padding (like \"half\" in some contexts)\n",
    "  #                            activation='tanh',\n",
    "  #                            name='Conv1D_Layer2')(conv_layer1)\n",
    "  # conv_layer3 = layers.Conv1D(filters=num_filters,\n",
    "  #                            kernel_size=kernel_size,\n",
    "  #                            strides=1,  # Step size for moving the filter along the input\n",
    "  #                            padding='same', # 'causal' gives left-side padding (like \"half\" in some contexts)\n",
    "  #                            activation='tanh',\n",
    "  #                            name='Conv1D_Layer3')(conv_layer2)\n",
    "\n",
    "  # Flatten Layer\n",
    "  flatten_layer = layers.Flatten(name='Flatten_Layer')(conv_layer1)\n",
    "\n",
    "  # Dense Hidden Layers\n",
    "  dense_layer_1 = layers.Dense(units=dnn_hidden_units,\n",
    "                               activation='relu',\n",
    "                               name='Dense_Hidden_1')(flatten_layer)\n",
    "  dense_layer_2 = layers.Dense(units=dnn_hidden_units,\n",
    "                               activation='tanh',\n",
    "                               name='Dense_Hidden_2')(dense_layer_1)\n",
    "  dense_layer_3 = layers.Dense(units=dnn_hidden_units,\n",
    "                               activation='tanh',\n",
    "                               name='dense_layer_3')(dense_layer_2)\n",
    "  dense_layer_4 = layers.Dense(units=dnn_hidden_units,\n",
    "                               activation='tanh',\n",
    "                               name='dense_layer_4')(dense_layer_3)\n",
    "  dense_layer_5 = layers.Dense(units=dnn_hidden_units,\n",
    "                                activation='tanh',\n",
    "                                name='dense_layer_5')(dense_layer_4)\n",
    "  \n",
    "\n",
    "  # Output Layer\n",
    "  output_tensor = layers.Dense(units=1,\n",
    "                               activation='linear', # For regression\n",
    "                               name='Output_Layer')(dense_layer_3)\n",
    "\n",
    "  # Define the model\n",
    "  model = keras.Model(inputs=input_tensor, outputs=output_tensor, name='MDA_CNN')\n",
    "\n",
    "  # Compile the model\n",
    "  model.compile(optimizer=keras.optimizers.Adam(),\n",
    "                loss='mean_squared_error', # MSE loss for regression\n",
    "                metrics=['mean_absolute_error']) # Optional: track MAE\n",
    "\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c8c1e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_mda_cnn2(input_table_shape, num_filters=32, kernel_size=30, dnn_hidden_units=10):\n",
    "    \"\"\"\n",
    "    Builds and compiles the MDA-CNN model.\n",
    "\n",
    "    Args:\n",
    "        input_table_shape (tuple): Shape of a single input table (N_L, C),\n",
    "                                e.g., (100, 4) for N_L=100, C=4 channels.\n",
    "        num_filters (int): Number of filters for the Conv1D layer.\n",
    "        kernel_size (int): Kernel size for the Conv1D layer.\n",
    "        dnn_hidden_units (int): Number of units in each hidden Dense layer.\n",
    "\n",
    "    Returns:\n",
    "        keras.Model: The compiled Keras model.\n",
    "    \"\"\"\n",
    "    # Input Layer\n",
    "    input_tensor = keras.Input(shape=input_table_shape, name='Input_Table')\n",
    "\n",
    "    # Convolutional Layer\n",
    "    # Using 'same' padding ensures the output length is N_L.\n",
    "    # 'relu' activation is a common choice for Conv layers.\n",
    "    conv_layer0 = layers.Conv1D(filters=num_filters,\n",
    "                                kernel_size=kernel_size,\n",
    "                                strides=2,\n",
    "                                padding='same', # Ensures output sequence length = N_L\n",
    "                                activation='relu',\n",
    "                                name='Conv1D_Layer0')(input_tensor)\n",
    "    # Second Convolutional Layer\n",
    "    # This layer can be used to further extract features from the output of the first Conv1D layer.\n",
    "    # Change padding to 'causal' for \"half\" padding effect (left-padding).\n",
    "    # Note: Keras does not have a 'half' padding mode, but 'causal' is the closest for 1D convs.\n",
    "    # If you want true \"half\" padding (as in some frameworks), you may need to implement a custom layer.\n",
    "\n",
    "    conv_layer1 = layers.Conv1D(filters=num_filters,\n",
    "                                kernel_size=kernel_size,\n",
    "                                strides=1,  # Step size for moving the filter along the input\n",
    "                                padding='same', # 'causal' gives left-side padding (like \"half\" in some contexts)\n",
    "                                activation='relu',\n",
    "                                name='Conv1D_Layer1')(conv_layer0)\n",
    "\n",
    "    # For 2D convolution, we can use Conv2D with a similar structure.\n",
    "    # Reshape the Conv1D output to 2D for Conv2D processing\n",
    "    # Reshape conv_layer1 into a 2D input, padding if needed\n",
    "    n_l = int(conv_layer1.shape[1])\n",
    "    reshape_size = int(np.ceil(np.sqrt(n_l)))\n",
    "    desired_n = reshape_size ** 2\n",
    "    pad_rows = desired_n - n_l\n",
    "\n",
    "    # Pad conv_layer1 along the sequence dimension if needed\n",
    "    if pad_rows > 0:\n",
    "        conv_layer1 = layers.ZeroPadding1D(padding=(0, pad_rows))(conv_layer1)\n",
    "\n",
    "    conv_layer1 = layers.Reshape((reshape_size, reshape_size, num_filters))(conv_layer1)\n",
    "\n",
    "    # Second Convolutional Layer\n",
    "    # This layer can be used to further extract features from the output of the first Conv2D layer.\n",
    "    conv2d_layer0 = layers.Conv2D(\n",
    "        filters=num_filters,\n",
    "        kernel_size=(3, 3),\n",
    "        strides=(1, 1),\n",
    "        padding='same',\n",
    "        activation='relu',\n",
    "        name='Conv2D_Layer0'\n",
    "    )(conv_layer1)\n",
    "\n",
    "    conv2d_layer1 = layers.Conv2D(\n",
    "        filters=num_filters,\n",
    "        kernel_size=(3, 3),\n",
    "        strides=(1, 1),\n",
    "        padding='same',\n",
    "        activation='relu',\n",
    "        name='Conv2D_Layer1'\n",
    "    )(conv2d_layer0)\n",
    "\n",
    "    conv2d_layer2 = layers.Conv2D(\n",
    "        filters=num_filters,\n",
    "        kernel_size=(3, 3),\n",
    "        strides=(1, 1),\n",
    "        padding='same',\n",
    "        activation='relu',\n",
    "        name='Conv2D_Layer2'\n",
    "    )(conv2d_layer1)\n",
    "    # Flatten Layer\n",
    "    flatten_layer = layers.Flatten(name='Flatten_Layer')(conv2d_layer2)\n",
    "\n",
    "    # Dense Hidden Layers\n",
    "    dense_layer_1 = layers.Dense(units=dnn_hidden_units,\n",
    "                                activation='tanh',\n",
    "                                name='Dense_Hidden_1')(flatten_layer)\n",
    "    dense_layer_2 = layers.Dense(units=dnn_hidden_units,\n",
    "                                activation='tanh',\n",
    "                                name='Dense_Hidden_2')(dense_layer_1)\n",
    "    dense_layer_3 = layers.Dense(units=dnn_hidden_units,\n",
    "                                activation='tanh',\n",
    "                                name='dense_layer_3')(dense_layer_2)\n",
    "    dense_layer_4 = layers.Dense(units=dnn_hidden_units,\n",
    "                                activation='tanh',\n",
    "                                name='dense_layer_4')(dense_layer_3)\n",
    "    dense_layer_5 = layers.Dense(units=dnn_hidden_units,\n",
    "                                    activation='tanh',\n",
    "                                    name='dense_layer_5')(dense_layer_4)\n",
    "    \n",
    "\n",
    "    # Output Layer\n",
    "    output_tensor = layers.Dense(units=1,\n",
    "                                activation='linear', # For regression\n",
    "                                name='Output_Layer')(dense_layer_3)\n",
    "\n",
    "    # Define the model\n",
    "    model = keras.Model(inputs=input_tensor, outputs=output_tensor, name='MDA_CNN2')\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer=keras.optimizers.Adam(),\n",
    "                    loss='mean_squared_error', # MSE loss for regression\n",
    "                    metrics=['mean_absolute_error']) # Optional: track MAE\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef3a9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.special import iv\n",
    "\n",
    "Q_x = np.array([\n",
    "    0.4,\n",
    "    0.485,\n",
    "    0.57,\n",
    "    0.655,\n",
    "    0.74,\n",
    "    0.825,\n",
    "    0.91,\n",
    "    0.995,\n",
    "    1.08,\n",
    "    1.165,\n",
    "    1.25,\n",
    "    1.335,\n",
    "    1.42,\n",
    "    1.505,\n",
    "    1.59,\n",
    "    1.675,\n",
    "    1.76,\n",
    "    1.845,\n",
    "    1.93,\n",
    "    2.015,\n",
    "    2.1\n",
    "])\n",
    "\n",
    "Q_H_y_3 = np.array([\n",
    "    0.6350203,\n",
    "    0.6041921,\n",
    "    0.579231,\n",
    "    0.5584657,\n",
    "    0.5409,\n",
    "    0.5258056,\n",
    "    0.5127053,\n",
    "    0.5012174,\n",
    "    0.4910923,\n",
    "    0.4820835,\n",
    "    0.4740534,\n",
    "    0.466857,\n",
    "    0.4604359,\n",
    "    0.4547031,\n",
    "    0.4495232,\n",
    "    0.4448568,\n",
    "    0.4406356,\n",
    "    0.4368164,\n",
    "    0.4333387,\n",
    "    0.4301594,\n",
    "    0.4272637\n",
    "])\n",
    "Q_H_y_20 = np.array([\n",
    "    0.446000,\n",
    "    0.431400,\n",
    "    0.419000,\n",
    "    0.408500,\n",
    "    0.399300,\n",
    "    0.391300,\n",
    "    0.384200,\n",
    "    0.378000,\n",
    "    0.372400,\n",
    "    0.367300,\n",
    "    0.362800,\n",
    "    0.358700,\n",
    "    0.355000,\n",
    "    0.351700,\n",
    "    0.348600,\n",
    "    0.345800,\n",
    "    0.343300,\n",
    "    0.341000,\n",
    "    0.338900,\n",
    "    0.336900,\n",
    "    0.335100\n",
    "])\n",
    "Q_L_y = np.array([\n",
    "    0.649120,\n",
    "    0.616192,\n",
    "    0.589531,\n",
    "    0.567466,\n",
    "    0.548800,\n",
    "    0.532806,\n",
    "    0.519005,\n",
    "    0.507017,\n",
    "    0.496392,\n",
    "    0.487084,\n",
    "    0.478853,\n",
    "    0.471557,\n",
    "    0.465036,\n",
    "    0.459203,\n",
    "    0.454023,\n",
    "    0.449257,\n",
    "    0.445036,\n",
    "    0.441216,\n",
    "    0.437839,\n",
    "    0.434759,\n",
    "0.431864\n",
    "])\n",
    "\n",
    "\n",
    "#def sabr_implied_vol(K, F=1, T=3, alpha=0.5, beta=0.6, rho=-0.2, nu=0.3):\n",
    "def sabr_implied_vol(K, T=20, F=1, alpha=0.5, beta=0.6, rho=-0.2, nu=0.3):\n",
    "    \"\"\"\n",
    "    Computes the SABR-implied Black volatility using Hagan's approximation.\n",
    "    \n",
    "    Parameters:\n",
    "        K (float): Strike price\n",
    "        F (float): Forward price\n",
    "        T (float): Time to maturity\n",
    "        alpha (float): SABR volatility parameter\n",
    "        beta (float): SABR elasticity parameter (0 <= beta <= 1)\n",
    "        rho (float): SABR correlation parameter (-1 <= rho <= 1)\n",
    "        nu (float): SABR volatility of volatility parameter\n",
    "\n",
    "    Returns:\n",
    "        float: The implied volatility (Black volatility)\n",
    "    \"\"\"\n",
    "    \n",
    "    K = np.asarray(K, dtype=np.float64)\n",
    "    eps = 1e-07\n",
    "    # Determine the ATM condition for each K\n",
    "    atm = np.abs(F - K) < eps\n",
    "\n",
    "    # ATM branch\n",
    "    vol_atm = (alpha / (F**(1-beta))) * (\n",
    "        1 + (\n",
    "            ((1-beta)**2 / 24) * (alpha**2 / (F**(2-2*beta))) +\n",
    "            (rho * beta * nu * alpha) / (4 * (F**(1-beta))) +\n",
    "            ((2-3*rho**2) / 24) * nu**2\n",
    "        ) * T\n",
    "    )\n",
    "\n",
    "    # Non-ATM branch\n",
    "    log_fk = np.log(F / K)\n",
    "    fk_beta = (F * K)**((1-beta)/2)\n",
    "    z = (nu / alpha) * fk_beta * log_fk\n",
    "    sqrt_expr = np.sqrt(1 - 2 * rho * z + z**2)\n",
    "    x_z = np.where(\n",
    "        np.abs(z) > eps,\n",
    "        np.log((sqrt_expr + z - rho) / (1 - rho)),\n",
    "        z - 0.5 * rho * z**2  # Series expansion for small z\n",
    "    )\n",
    "    term1 = ((1-beta)**2 / 24) * (alpha**2 / (fk_beta**2))\n",
    "    term2 = (rho * beta * nu * alpha) / (4 * fk_beta)\n",
    "    term3 = ((2-3*rho**2) / 24) * nu**2\n",
    "    vol_nonatm = (alpha / (((F * K)**((1-beta)/2)) * (1 + ((1-beta)**2/24)*(log_fk**2) + ((1-beta)**4/1920)*(log_fk**4)))) \\\n",
    "                 * (z / x_z) * (1 + (term1 + term2 + term3) * T)\n",
    "\n",
    "    vol = np.where(atm, vol_atm, vol_nonatm)\n",
    "    # Return a scalar if input was scalar\n",
    "    if vol.size == 1:\n",
    "        return float(vol)\n",
    "    return vol\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def sabr_xzz_pdf(K, T=20, F=1, alpha=0.5, beta=0.5, rho=-0.2, nu=0.3):\n",
    "    \"\"\"\n",
    "    Computes the SABR-implied Black volatility using Hagan's approximation.\n",
    "    \n",
    "    Parameters:\n",
    "        K (float): Strike price\n",
    "        F (float): Forward price\n",
    "        T (float): Time to maturity\n",
    "        alpha (float): SABR volatility parameter\n",
    "        beta (float): SABR elasticity parameter (0 <= beta <= 1)\n",
    "        rho (float): SABR correlation parameter (-1 <= rho <= 1)\n",
    "        nu (float): SABR volatility of volatility parameter\n",
    "\n",
    "    Returns:\n",
    "        float: The implied volatility (Black volatility)\n",
    "    \"\"\"\n",
    "    # Implementation details for the SABR XZZ model would go here\n",
    "    r = 0.5/(1-beta)\n",
    "    def Z(f):\n",
    "        return f**(1-beta)/alpha/(1-beta)\n",
    "    def J(z):\n",
    "        return (1 - 2*rho*nu*z + (nu*z)**2)**0.5\n",
    "    def X(z):\n",
    "        return np.log((J(z) + z*nu - rho)/(1 - rho))/nu \n",
    "    def bessel(z):\n",
    "        return iv(r, z)\n",
    "    def K1(z):\n",
    "        zf = Z(K) + z\n",
    "        return 1/8*(2-3*(rho-nu*z)**2/J(z)**2)*nu*nu-1/4*rho*beta*nu/(1-beta)/zf\n",
    "    def K2(z):\n",
    "        zf = Z(K) + z\n",
    "        return -beta*(2-beta)/8/(1-beta)**2/(zf**2)\n",
    "    \n",
    "    if(beta ==1):\n",
    "        z = np.log(F/K)/alpha\n",
    "    else:\n",
    "        z = Z(F)-Z(K)\n",
    "    def h(z):\n",
    "        rho_=np.sqrt(1-rho*rho)\n",
    "        return 1/2*beta*rho/(1-beta)/J(-Z(K))**2*(nu*Z(K)*np.log(Z(K)*J(z)/Z(F))+(1+rho*nu*Z(K))/rho_*(np.arctan((nu*z-rho)/rho_)+np.arctan(rho/rho_)))\n",
    "    \n",
    "    if (beta == 0):\n",
    "        pdf = 1/(alpha*K**beta)*J(z)**(-1.5)/np.sqrt(2*np.pi*T)*np.sqrt(F**beta/K**beta)*np.exp(-X(z)**2/2/T)\n",
    "    if beta ==1:\n",
    "        pdf = 1/(alpha*J(z)**(-1.5)*np.sqrt(2*np.pi*T*F*K))*np.exp(-X(z)**2/2/T)\n",
    "    else:\n",
    "        pdf = 1/(alpha*K**beta)*J(z)**(-1.5)/T*Z(F)**r*Z(K)**(1-r)*bessel(Z(F)*Z(K)/T)*np.exp(-X(z)**2/2/T-Z(F)*Z(K)/T+h(z)+(K1(z/2))*T)\n",
    "    return pdf\n",
    "\n",
    "\n",
    "from scipy.integrate import quad\n",
    "def integrate_pdf(T=20, F=1, alpha=0.5, beta = 0.5, rho=-0.2, nu=0.3):\n",
    "    integrand = lambda K: sabr_xzz_pdf(K, T=T, F=F, alpha=alpha, beta=beta, rho=rho, nu=nu)*K\n",
    "    result, error = quad(integrand, 0, 10*F, limit=2000)\n",
    "    return result, error\n",
    "\n",
    "\n",
    "def call_price_sabr(K, T, F, alpha, beta, rho, nu,scale):\n",
    "    # K can be an array, so we need to return an array of call prices\n",
    "    # Use np.vectorize to apply integration for each strike K\n",
    "    def integrand(f, K_):\n",
    "        return sabr_xzz_pdf(f, T, F, alpha, beta, rho, nu) * scale * np.maximum(f - K_, 0)\n",
    "    # Vectorized integration for each K in K_values\n",
    "    if isinstance(K, np.ndarray):\n",
    "        call_prices = np.array([\n",
    "            quad(integrand, 0, 10*F, args=(k,), limit=2000)[0] for k in K\n",
    "        ])\n",
    "        return call_prices\n",
    "    else:\n",
    "        call_price, _ = quad(integrand, 0, 10*F, args=(K,), limit=2000)\n",
    "        return call_price\n",
    "\n",
    "from scipy.stats import norm\n",
    "from scipy.optimize import brentq\n",
    "def black_scholes_call(S, K, T, r, sigma):\n",
    "    \"\"\"\n",
    "    Calculates the Black-Scholes price for a European call option.\n",
    "\n",
    "    Args:\n",
    "        S (float): The current price of the underlying asset.\n",
    "        K (float): The strike price of the option.\n",
    "        T (float): The time to expiration in years.\n",
    "        r (float): The risk-free interest rate.\n",
    "        sigma (float): The volatility of the underlying asset.\n",
    "\n",
    "    Returns:\n",
    "        float: The price of the call option.\n",
    "    \"\"\"\n",
    "    # d1 and d2 are intermediate values used in the Black-Scholes formula\n",
    "    d1 = (np.log(S / K) + (r + 0.5 * sigma**2) * T) / (sigma * np.sqrt(T))\n",
    "    d2 = d1 - sigma * np.sqrt(T)\n",
    "    \n",
    "    # Calculate the call option price\n",
    "    call_price = (S * norm.cdf(d1) - K * np.exp(-r * T) * norm.cdf(d2))\n",
    "    return call_price\n",
    "\n",
    "def vega(S, K, T, r, sigma):\n",
    "    \"\"\"\n",
    "    Calculates the Vega of an option. Vega measures the sensitivity of an \n",
    "    option's price to changes in the volatility of the underlying asset.\n",
    "\n",
    "    Args:\n",
    "        S (float): The current price of the underlying asset.\n",
    "        K (float): The strike price of the option.\n",
    "        T (float): The time to expiration in years.\n",
    "        r (float): The risk-free interest rate.\n",
    "        sigma (float): The volatility of the underlying asset.\n",
    "\n",
    "    Returns:\n",
    "        float: The Vega of the option.\n",
    "    \"\"\"\n",
    "    # d1 is an intermediate value used in the Black-Scholes formula\n",
    "    d1 = (np.log(S / K) + (r + 0.5 * sigma**2) * T) / (sigma * np.sqrt(T))\n",
    "    \n",
    "    # Calculate Vega\n",
    "    vega = S * norm.pdf(d1) * np.sqrt(T)\n",
    "    return vega\n",
    "\n",
    "def implied_volatility(call_price, S, K, T, r):\n",
    "    \"\"\"\n",
    "    Calculates the implied volatility for a European call option.\n",
    "\n",
    "    Args:\n",
    "        call_price (float): The market price of the call option.\n",
    "        S (float): Current stock price.\n",
    "        K (float): Strike price.\n",
    "        T (float): Time to maturity.\n",
    "        r (float): Risk-free interest rate.\n",
    "\n",
    "    Returns:\n",
    "        float: The implied volatility.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Objective function: difference between Black-Scholes price and market price\n",
    "        def objective_function(sigma):\n",
    "            return black_scholes_call(S, K, T, r, sigma) - call_price\n",
    "\n",
    "        # Use Brent's method to find the root (the implied volatility)\n",
    "        # We provide a reasonable bracket for volatility [0.001, 2.0]\n",
    "        return brentq(objective_function, 1e-3, 2.0)\n",
    "    except ValueError:\n",
    "        # If the solver fails, it may be because the price is outside the no-arbitrage bounds\n",
    "        return np.nan\n",
    "    \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "K_values = np.linspace(0.1, 2, 20)\n",
    "\n",
    "betas = np.arange(0., 1.0, 10)\n",
    "F = 1.0\n",
    "T = .5\n",
    "alpha0 = 0.25\n",
    "beta = 0.6\n",
    "nu = 0.3\n",
    "rho = -0.8\n",
    "integral = integrate_pdf(T=T, F=F, alpha=alpha0, beta=beta, rho=rho, nu=nu)[0]\n",
    "scale = F/integral\n",
    "price = call_price_sabr(K_values, T=T, F=F, alpha=alpha0, beta=beta, rho=rho, nu=nu, scale=scale)\n",
    "# Calculate implied volatility for a range of strike prices\n",
    "for k, p in zip(K_values, price):\n",
    "    v = implied_volatility(call_price=p, S=F, K=k, T=T, r=0.0)\n",
    "    print(f\"Strike: {k:.4f}, Price: {p:.6f}, Vol: {v:.6f}\")\n",
    "# Plot the call prices\n",
    "\n",
    "for beta in betas:\n",
    "    vols_beta = sabr_xzz_pdf(K_values, T=T, F=F, alpha=alpha0, beta=beta, rho=rho, nu=nu)*K_values\n",
    "    plt.figure()\n",
    "    plt.plot(K_values[1:], vols_beta[1:], label=f'beta={beta:.1f}')\n",
    "    plt.title(f\"SABR XZZ PDF, beta={beta:.2f}\")\n",
    "    \n",
    "    # Integrate sabr_xzz_pdf over K_values for the given beta\n",
    "    integral = integrate_pdf(T=T, F=F, alpha=alpha0, beta=beta, rho=rho, nu=nu)\n",
    "    print(f\"Integral of sabr_xzz_pdf over K_values for beta={beta}: {integral}\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7999c567",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.interpolate import CubicSpline\n",
    "\n",
    "# --- Assume functions from previous sections are defined: ---\n",
    "# normalize_data(data, qh_min, qh_max)\n",
    "# denormalize_data(normalized_data, qh_min, qh_max)\n",
    "# compile_input_table(hf_y, hf_q_l, lf_y, lf_q)\n",
    "# build_mda_cnn(input_table_shape, num_filters=64, kernel_size=3, dnn_hidden_units=10)\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "# Step 5a: Generate Synthetic Data\n",
    "def Q_L(y):\n",
    "  \"\"\"Low-fidelity function (e.g., sine wave).\"\"\"\n",
    "  return np.sin(8*np.pi*y)\n",
    "  #q = 0.5*(6*y-2)**2*np.sin(12 * y-4) +10*(y-0.5)-5\n",
    "  #return q\n",
    "  #return np.where(y <= 0.5, q, q + 3)\n",
    "  #return np.sin(2 * np.pi * y) + (y - 0.5) # Example with more structure\n",
    "  #return np.sin(2 * np.pi * y) - 0.5 * (y - 0.5) # Example with more structure\n",
    "def Q_H(y):\n",
    "  \"\"\"High-fidelity function (e.g., LF + correction term).\"\"\"\n",
    "  #return (6*y-2)**2*np.sin(12 * y-4) -10*(y-1)**2\n",
    "  # q = 2*Q_L(y)-20*y+20\n",
    "  # return np.where(y <= 0.5, q, q + 4)\n",
    "  #return np.sin(y) + 0.1 * np.sin(10 * y)\n",
    "  #return (y - np.sqrt(2)) * Q_L(y)**2 # Alternative example\n",
    "  return y**2+ Q_L(y+np.pi/10)**2 # Alternative example\n",
    "  #return Q_L(y) * 1.2 + 0.1 * np.cos(10 * np.pi * y) + 0.1*y # Example related to Q_L\n",
    "\n",
    "# Define data ranges and sizes\n",
    "y_min, y_max = 0, 1\n",
    "n_l_samples = 100  # Number of low-fidelity samples\n",
    "n_h_samples = 10   # Number of high-fidelity samples (sparse)\n",
    "\n",
    "# Generate LF data (dense grid)\n",
    "lf_y_all = np.linspace(y_min, y_max, n_l_samples)\n",
    "\n",
    "lf_q_all = Q_L(lf_y_all)\n",
    "\n",
    "# Generate HF data (sparse, randomly sampled)\n",
    "# Ensure HF points are within the LF range for interpolation later\n",
    "hf_y_train = np.linspace(y_min, y_max, n_h_samples)\n",
    "#hf_y_train = np.sort(np.random.uniform(y_min, y_max, n_h_samples))\n",
    "hf_q_train = Q_H(hf_y_train)\n",
    "hf_q_l_train = Q_L(hf_y_train)\n",
    "\n",
    "# test sabr\n",
    "Q_H_y = Q_H_y_3  # Use the 20-year high-fidelity data for testing\n",
    "T = 3\n",
    "y_min, y_max = 0.4, 2.1\n",
    "n_l_samples = 40\n",
    "n_h_samples = 5\n",
    "\n",
    "lf_y_all = np.linspace(y_min-0.1, y_max+0.5, n_l_samples)\n",
    "#lf_y_all = np.random.uniform(y_min-0.1, y_max+0.5, n_l_samples)\n",
    "\n",
    "lf_q_all = sabr_implied_vol(lf_y_all,T)\n",
    "\n",
    "# lf_y_all = Q_x\n",
    "# lf_q_all = Q_L_y\n",
    "# n_l_samples = len(lf_y_all)\n",
    "\n",
    "# Randomly pick 5 indices from lf_y_all\n",
    "# Always include the first (0) and last (len(Q_x)-1) indices, then randomly pick the remaining\n",
    "remaining_indices = np.setdiff1d(np.arange(len(Q_x)), [0, len(Q_x)-1])\n",
    "rand_indices_rest = np.random.choice(remaining_indices, size=n_h_samples-2, replace=False)\n",
    "rand_indices = np.concatenate(([0], rand_indices_rest, [len(Q_x)-1]))\n",
    "\n",
    "#rand_indices = np.random.choice(np.arange(len(Q_x)), size=n_h_samples, replace=False)\n",
    "rand_indices = np.linspace(0, len(Q_x)-1, n_h_samples, dtype=int)\n",
    "\n",
    "hf_y_train = Q_x[rand_indices]\n",
    "hf_q_train0 = Q_H_y[rand_indices]\n",
    "#hf_q_l_train = Q_L_y[rand_indices]\n",
    "hf_q_l_train = sabr_implied_vol(hf_y_train,T)\n",
    "hf_q_train = hf_q_train0-hf_q_l_train\n",
    "\n",
    "\n",
    "\n",
    "print(f\"Generated {n_l_samples} LF samples.\")\n",
    "print(f\"Generated {n_h_samples} HF samples for training.\")\n",
    "\n",
    "# Step 5b: Calculate HF Range for Normalization\n",
    "qh_min = np.min(hf_q_train)\n",
    "qh_max = np.max(hf_q_train)\n",
    "#qh_min = np.min(lf_q_all)\n",
    "#qh_max = np.max(lf_q_all)\n",
    "print(f\"HF Training Data Range (Q_H): min={qh_min:.4f}, max={qh_max:.4f}\")\n",
    "\n",
    "# Step 5c: Normalize Data\n",
    "# Normalize HF training outputs (target variable)\n",
    "y_train_normalized = normalize_data(hf_q_train, qh_min, qh_max)\n",
    "\n",
    "# Normalize ALL LF outputs using the HF range\n",
    "lf_q_all_normalized = normalize_data(lf_q_all, qh_min, qh_max)\n",
    "\n",
    "print(\"Normalized HF and LF data using HF range.\")\n",
    "\n",
    "# Step 5d: Compile Input Tables for Training\n",
    "X_train_list =[]\n",
    "for i in range(n_h_samples):\n",
    "  hf_y_i = hf_y_train[i]\n",
    "\n",
    "  # Get the corresponding LF prediction AT hf_y_i.\n",
    "  # Since lf_y_all is a dense grid, we can interpolate.\n",
    "  # IMPORTANT: Use the *normalized* LF data for interpolation here.\n",
    "  \n",
    "  #hf_q_l_i_normalized = np.interp(hf_y_i, lf_y_all, lf_q_all_normalized)\n",
    "  hf_q_l_i_normalized = normalize_data(hf_q_l_train[i], qh_min, qh_max)\n",
    "\n",
    "  # Compile the table for this HF point using the full LF dataset context\n",
    "  table_i = compile_input_table(hf_y_i, hf_q_l_i_normalized, lf_y_all, lf_q_all_normalized)\n",
    "  X_train_list.append(table_i)\n",
    "\n",
    "# Stack the tables to create the training input tensor\n",
    "# Shape: (N_H, N_L, C) -> (n_h_samples, n_l_samples, 4) for 1D case\n",
    "X_train = np.array(X_train_list)\n",
    "y_train = y_train_normalized # Target variable\n",
    "\n",
    "print(f\"Compiled training input X_train with shape: {X_train.shape}\")\n",
    "print(f\"Training target y_train with shape: {y_train.shape}\")\n",
    "\n",
    "# Step 5e: Build the Model\n",
    "input_table_shape = X_train.shape[1:] # (N_L, C)\n",
    "model = build_mda_cnn(input_table_shape, num_filters=32, kernel_size=3, dnn_hidden_units=5) # Example: Adjusted hyperparameters\n",
    "model.summary()\n",
    "\n",
    "# Step 5f: Train the Model\n",
    "print(\"\\nStarting model training...\")\n",
    "history = model.fit(X_train, y_train,\n",
    "                    epochs=2600, # Adjust epochs as needed\n",
    "                    batch_size=32, # Adjust batch size based on N_H\n",
    "                    verbose=1) # Set verbose=1 or 2 to see progress\n",
    "print(\"Model training complete.\")\n",
    "print(f\"Final training loss: {history.history['loss'][-1]:.4f}\")\n",
    "\n",
    "# Step 5g: Prediction Example\n",
    "# Choose a new point for prediction (not in hf_y_train)\n",
    "y_pred_point = 0.65\n",
    "\n",
    "# 1. Get LF prediction at the new point\n",
    "lf_q_pred = Q_L(y_pred_point)\n",
    "\n",
    "# 2. Normalize the LF prediction using the SAME HF range\n",
    "lf_q_pred_normalized = normalize_data(lf_q_pred, qh_min, qh_max)\n",
    "\n",
    "# 3. Compile the input table for the prediction point.\n",
    "#    *** CRITICAL: Use the original full lf_y_all and lf_q_all_normalized ***\n",
    "#    The model relies on the context learned from this specific LF dataset.\n",
    "# Compile the input table for the prediction point\n",
    "X_pred_table = compile_input_table(y_pred_point, lf_q_pred_normalized, lf_y_all, lf_q_all_normalized)\n",
    "\n",
    "# Debug: Print the shape of the prediction table\n",
    "print(f\"X_pred_table shape: {X_pred_table.shape}\")\n",
    "\n",
    "# Reshape the table for model prediction (add batch dimension)\n",
    "X_pred_input = np.expand_dims(X_pred_table, axis=0)  # Shape: (1, N_L, C)\n",
    "\n",
    "# Debug: Print the shape of the input to the model\n",
    "print(f\"X_pred_input shape: {X_pred_input.shape}\")\n",
    "\n",
    "# Make the prediction (output is normalized)\n",
    "y_pred_normalized = model.predict(X_pred_input)\n",
    "\n",
    "# Debug: Print the normalized prediction\n",
    "print(f\"y_pred_normalized: {y_pred_normalized}\")\n",
    "\n",
    "# Denormalize the prediction to get the final value\n",
    "y_pred_denormalized = denormalize_data(y_pred_normalized, qh_min, qh_max)\n",
    "\n",
    "# Debug: Print the denormalized prediction\n",
    "print(f\"y_pred_denormalized: {y_pred_denormalized}\")\n",
    "\n",
    "# Compare with true value\n",
    "true_q_h_at_pred = Q_H(y_pred_point)\n",
    "\n",
    "print(f\"\\nPrediction at y = {y_pred_point:.4f}:\")\n",
    "print(f\"  Normalized LF prediction (hf_q_l_norm): {lf_q_pred_normalized:.4f}\")\n",
    "print(f\"  Model normalized output (y_pred_norm): {y_pred_normalized[0][0]:.4f}\")\n",
    "print(f\"  Denormalized Prediction (Q_H_pred): {y_pred_denormalized[0][0]:.4f}\")\n",
    "print(f\"  True High-Fidelity Value (Q_H_true): {true_q_h_at_pred:.4f}\")\n",
    "print(f\"  Prediction Error: {abs(y_pred_denormalized[0][0] - true_q_h_at_pred):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7fecfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit hf_y_train, hf_q_train using cubic spline interpolation\n",
    "# Sort hf_y_train and hf_q_train so the x-values are strictly increasing\n",
    "sort_idx = np.argsort(hf_y_train)\n",
    "hf_y_train_sorted = hf_y_train[sort_idx]\n",
    "hf_q_train_sorted = hf_q_train[sort_idx]\n",
    "\n",
    "# Create a cubic spline interpolator for the high-fidelity data\n",
    "cs = CubicSpline(hf_y_train_sorted, hf_q_train_sorted)\n",
    "# Generate a dense grid of points for plotting\n",
    "y_dense = Q_x\n",
    "# Evaluate the spline on the dense grid\n",
    "hf_q_dense = cs(y_dense)+ sabr_implied_vol(y_dense,T)\n",
    "\n",
    "#y_plot = np.linspace(y_min, y_max, 200)\n",
    "y_plot = Q_x\n",
    "# Optional: Plot results\n",
    "plt.figure(figsize=(10, 6))\n",
    "#plt.plot(y_plot, Q_H(y_plot), 'k--', label='True $Q_H(y)$', alpha=0.7)\n",
    "# Sort lf_y_all and lf_q_all together based on lf_y_all for plotting\n",
    "lf_sort_idx = np.argsort(lf_y_all)\n",
    "lf_y_all_sorted = lf_y_all[lf_sort_idx]\n",
    "lf_q_all_sorted = lf_q_all[lf_sort_idx]\n",
    "plt.plot(lf_y_all_sorted, lf_q_all_sorted, 'b:', label='$Q_L(y)$ (sorted)', alpha=0.7)\n",
    "\n",
    "#plt.plot(lf_y_all, lf_q_all, 'b:', label='$Q_L(y)$', alpha=0.7)\n",
    "plt.plot(y_plot, Q_H_y, 'k--', label='True $Q_H(y)$', alpha=0.7)\n",
    "plt.scatter(hf_y_train, hf_q_train0, c='r', marker='o', s=80, label='HF Training Data ($Q_H$)')\n",
    "\n",
    "plt.plot(y_dense, hf_q_dense, 'r-', label='Cubic Spline Fit to HF Data', alpha=0.5)\n",
    "# Generate predictions across the domain for visualization\n",
    "\n",
    "q_h_pred_plot = []\n",
    "for i , y_p in  enumerate(y_plot):\n",
    "    lf_q_p = sabr_implied_vol(y_p, T)\n",
    "    #lf_q_p = lf_q_all[i]  # Low-fidelity prediction at y_p\n",
    "    lf_q_p_norm = normalize_data(lf_q_p, qh_min, qh_max)\n",
    "    table_p = compile_input_table(y_p, lf_q_p_norm, lf_y_all, lf_q_all_normalized)\n",
    "    input_p = np.expand_dims(table_p, axis=0)\n",
    "\n",
    "    # Debug: Print shapes during the loop\n",
    "    print(f\"y_p: {y_p}, table_p shape: {table_p.shape}, input_p shape: {input_p.shape}\")\n",
    "\n",
    "    pred_norm = model.predict(input_p, verbose=0)\n",
    "    pred_denorm = denormalize_data(pred_norm, qh_min, qh_max)\n",
    "    pred_denorm += lf_q_p\n",
    "    q_h_pred_plot.append(pred_denorm[0][0])  # Append the scalar value\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.plot(y_plot, q_h_pred_plot, 'g-', label='MDA-CNN Prediction')\n",
    "plt.scatter([y_pred_point], [y_pred_denormalized], c='lime', marker='x', s=100, zorder=5, label=f'Prediction at y={y_pred_point:.2f}')\n",
    "\n",
    "plt.xlabel('Input Parameter y')\n",
    "plt.ylabel('Quantity of Interest Q')\n",
    "plt.title('MDA-CNN Multi-fidelity Prediction Example')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "#plt.ylim(min(np.min(Q_H(lf_y_all)), np.min(lf_q_all)) - 0.2, max(np.max(Q_H(lf_y_all)), np.max(lf_q_all)) + 0.2) # Adjust ylim for better view\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(y_plot, 100*(np.array(q_h_pred_plot) - Q_H_y), marker='o', linestyle='-', color='purple', label='MDA-CNN Prediction Error ($Q_H - Q_H$)')\n",
    "plt.plot(y_plot, 100*(hf_q_dense - Q_H_y), marker='o', linestyle='-', color='orange', label='Cubic Spline Fit Error')\n",
    "#plt.plot(y_plot, 100*(Q_L_y - Q_H_y), marker='x', linestyle='-', color='blue', label='Low-Fidelity Prediction Error ($Q_L - Q_H$)')\n",
    "\n",
    "plt.scatter(hf_y_train, 100*(hf_q_train - hf_q_train), c='r', marker='o', s=80, label='HF Training Data ($Q_H$)')\n",
    "\n",
    "plt.axhline(0, color='gray', linestyle='--', linewidth=1)\n",
    "plt.xlabel('Input Parameter y')\n",
    "plt.ylabel('Prediction Error (Prediction - True $Q_H$)')\n",
    "plt.title('Prediction Error of MDA-CNN vs True High-Fidelity Data')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
